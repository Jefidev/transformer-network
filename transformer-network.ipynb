{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "obvious-starter",
   "metadata": {},
   "source": [
    "# Introduction to Transformer Networks\n",
    "\n",
    "Transformer Network were introduced in 2017 in the paper [**Attention Is All You Need**](https://arxiv.org/abs/1706.03762). They are designed to handle sequential data (such as text) and could be use in sequence to sequence tasks. Unlike the RNN methods, Transformer Networks does not needs to see the inputs sequentially allowing them to be trained in parallel. Thus, Transformer Networks became the model of choice for NLP as it is now possible to train them on very large datasets.\n",
    "\n",
    "This Notebook aims to show, in practice, how we move from RNN to Transformer Networks for a Sequence to Sequence (Seq2Seq) task. The task chosen is language translation. This Notebook showcase three different methods :\n",
    "\n",
    "- **Seq2Seq RNN** : Our baseline model for language translation.\n",
    "- **Seq2Seq RNN with Attention Mecanism** : An improved version of the RNN model.\n",
    "- **Transformer Network** : The model introduced in *Attention Is All You Need* dropping the RNN part of the network and keeping only the attention mecanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focused-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 5000\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "# Simple RNN : https://keras.io/examples/nlp/lstm_seq2seq/ ou https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350\n",
    "# RNN + Attention : https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# Transformer : https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-lobby",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The model we showcase are trained for english to french translation. It is a classical Seq2Seq problem for which a lot of datasets exists. We chose to use the data available on the [manythings.org](https://www.manythings.org/anki/) website.\n",
    "\n",
    "First we load language models from [**Spacy**](https://spacy.io/), a python library for NLP. These language model are used to tokenize each sentences in our dataset. We also use the [**Torchtext**](https://pytorch.org/text/stable/index.html) library as it provides useful class for loading text datasets and generating the vocabulary.\n",
    "\n",
    "Once preprocessed, the sentences start with a special start of sentence token (*<sos>*) and end of sentence token (*<eos>*) and are transformed into a vector of numbers each number corresponding to one word in the vocabulary of the language.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IWSLT2016\n",
    "import spacy\n",
    "import collections\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# Creating the tokenizer\n",
    "spacy_french = spacy.load(\"fr_core_news_sm\")\n",
    "spacy_english = spacy.load(\"en_core_web_sm\")\n",
    "MAX_LEN = 70\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "def tokenize_french(text):\n",
    "    return [token.text for token in spacy_french.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)]\n",
    "\n",
    "# Loading dataset\n",
    "train_iter, valid_iter, test_iter = IWSLT2016(language_pair=('fr', 'en'), valid_set='tst2013', test_set='tst2014')\n",
    "\n",
    "# Generate vocabulary\n",
    "en_c = collections.Counter()\n",
    "fr_c = collections.Counter()\n",
    "\n",
    "for fr, en in train_iter:\n",
    "    \n",
    "    if len(fr.split(\" \")) > MAX_LEN:\n",
    "        continue\n",
    "    \n",
    "    t_f = tokenize_french(fr)\n",
    "    fr_c.update(t_f)\n",
    "    \n",
    "    t_e = tokenize_english(en)\n",
    "    en_c.update(t_e)\n",
    "    train_data.append((fr, en))\n",
    "\n",
    "for fr, en in test_iter:\n",
    "    if len(fr.split(\" \")) > MAX_LEN:\n",
    "        continue\n",
    "        \n",
    "    test_data.append((fr, en))\n",
    "    \n",
    "\n",
    "voc_french = Vocab(fr_c, min_freq=20, specials=('<unk>', '<sos>', '<eos>', '<pad>'))\n",
    "voc_english = Vocab(en_c, min_freq=20, specials=('<unk>', '<sos>', '<eos>', '<pad>'))\n",
    "\n",
    "print(\"French vocabulary : \" + str(len(voc_french)))\n",
    "print(\"English vocabulary : \" + str(len(voc_english)))\n",
    "\n",
    "french_transform = lambda x: [voc_french['<sos>']] + [voc_french[token] for token in tokenize_french(x)] + [voc_french['<eos>']]\n",
    "english_transform = lambda x: [voc_english['<sos>']] + [voc_english[token] for token in tokenize_english(x)] + [voc_english['<eos>']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "secure-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \n",
    "    src = []\n",
    "    tgt = []\n",
    "    \n",
    "    \n",
    "    fr_pad = voc_french[\"<pad>\"]\n",
    "    en_pad = voc_english[\"<pad>\"]\n",
    "    \n",
    "    sequence_max_size = 0\n",
    "\n",
    "    for i, (_src, _tgt) in enumerate(batch):\n",
    "        \n",
    "        encoded_sentence = french_transform(_src)\n",
    "        sentence = torch.zeros(len(encoded_sentence))\n",
    "        for i, word_idx in enumerate(encoded_sentence):\n",
    "            sentence[i] = word_idx\n",
    "        \n",
    "            \n",
    "        src.append(sentence)\n",
    "        \n",
    "        tmp_text = english_transform(_tgt)\n",
    "        sentence = torch.zeros(len(tmp_text))\n",
    "        for i, word_idx in enumerate(tmp_text):\n",
    "            sentence[i] = word_idx\n",
    "            \n",
    "        tgt.append(sentence)\n",
    "        \n",
    "    # pad stuff\n",
    "    src = pad_sequence(src, padding_value=fr_pad)\n",
    "    tgt = pad_sequence(tgt, padding_value=en_pad)\n",
    "    \n",
    "    # src to one_hot\n",
    "    src_one_hot = torch.zeros(src.size()[0], BATCH_SIZE, len(voc_french))\n",
    "    \n",
    "    for i, tensor in enumerate(src):\n",
    "        for j, val in enumerate(tensor):\n",
    "            val = int(val.item())\n",
    "            src_one_hot[i][j][val] = 1\n",
    "    \n",
    "    return  src.to(torch.int64), tgt.to(torch.int64)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, \n",
    "                              collate_fn=collate_batch)\n",
    "\n",
    "test_dataloader = DataLoader(test_data[:100], batch_size = BATCH_SIZE, shuffle=True, \n",
    "                              collate_fn=collate_batch)\n",
    "\n",
    "for i in train_dataloader:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ignored-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "French -  Sans eau, les bactéries ne peuvent pas survivre.\n",
      "  Length -  49\n",
      "English -  Without water, the bacteria won't survive.\n",
      "  Length -  43\n",
      "French -  Et ça pourrait aussi être des choses qui ont besoin d'être auto-nettoyantes.\n",
      "  Length -  77\n",
      "English -  And it could be things that need to be self-cleaning as well.\n",
      "  Length -  62\n",
      "French -  Maintenant imaginez dans quelle mesure une telle chose pourrait aider à révolutionner votre domaine de travail.\n",
      "  Length -  112\n",
      "English -  So imagine how something like this could help revolutionize your field of work.\n",
      "  Length -  80\n",
      "French -  Je vais vous laisser avec une dernière démonstration, mais avant cela, je voudrais vous dire merci, et pensez petit.\n",
      "  Length -  117\n",
      "English -  And I'm going to leave you with one last demonstration, but before I do that, I would like to say thank you, and think small.\n",
      "  Length -  126\n",
      "Maximum Length of English Sentence 471 and French Sentence 508 in the dataset\n",
      "Minimum Length of English Sentence 2 and French Sentence 2 in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Creating the iterator and print sample\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "max_len_fr = []\n",
    "max_len_en = []\n",
    "count = 0\n",
    "\n",
    "for data in train_data:\n",
    "    max_len_en.append(len(data[1]))\n",
    "    max_len_fr.append(len(data[0]))\n",
    "    if count > 1000 and count < 1005 :\n",
    "        print(\"French - \", data[0] , \" Length - \", len(data[0]))\n",
    "        print(\"English - \",data[1], \" Length - \", len(data[1]))\n",
    "    count += 1\n",
    "\n",
    "print(\"Maximum Length of English Sentence {} and French Sentence {} in the dataset\".format(max(max_len_en),max(max_len_fr)))\n",
    "print(\"Minimum Length of English Sentence {} and French Sentence {} in the dataset\".format(min(max_len_en),min(max_len_fr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-composer",
   "metadata": {},
   "source": [
    "# Seq2Seq RNN\n",
    "\n",
    "A Seq2Seq RNN is made of two component. An encoder network evaluating the input sequence and generating a vector representing the sentence called the **Context Vector**. The context vector is then passed to a decoder network that will construct the input sequence.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The input of the encoder is the tokenized sentence with the start and end of sentence token. The purpose of the encoder is to create a context vector containing all the information needed by the decoder to reconstitute the translated sentence. To process the sequence of word token, we use LSTM layer and the hidden state of the last LSTM layer is used as the context vector.\n",
    "\n",
    "![](./fig/seq2seq-encoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "editorial-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        # Size of the input vector\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Size of the word embedding\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # LSTM hidden layer size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Number of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initializing the network layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=dropout_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x)\n",
    "        \n",
    "        return hidden_state, cell_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-finger",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "As the context vector issued by the encoder are the hidden state of LSTM, the decoder could use those context vector as the initial hidden state of its LSTM unit. In order to predict the next word in the sentence, the decoder network could use the information contained in the hiddent state of its LSTM units and the previous word it predicted :\n",
    "\n",
    "![](./fig/seq2seq-decoder.png)\n",
    "\n",
    "The first call of the decoder is initialize with the context vector and the start of sentence token. Then, the decoder use the previous token it emits as an input and could rely on its hidden state to convey the remaining contextual information about the sentence. The decoder is expected to emit the end of sentence token once he reached the end of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developmental-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        \n",
    "        # Input size of the decoder (size of the context vector)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Embedding size \n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Hidden unit size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # num of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Vocabulary size of the target language\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = dropout_p)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \n",
    "        # Shape of [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x, (hidden_state, cell_state))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden_state, cell_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-enough",
   "metadata": {},
   "source": [
    "## Joining Encoder and Decoder\n",
    "\n",
    "The final Seq2seq model could be built by stacking the encoder and the decoder network. But a regularization mecanism is added in the final seq2seq model to ease the learning tasks. The regularization is called the **Teach Force Ration** (tfr) and aims to correct the decoder by providing the actual token from the target sentence as an input instead of reusing the previously predicted token. Here is an example of the full seq2seq network :\n",
    "\n",
    "![](./fig/seq2seq.png)\n",
    "\n",
    "We just pass the context vector as the initial hidden state of the decoder and add the TFR mecanism that consist of providing the real target token expected as input of the LSTM instead of the previously predicted token with a probability of 50%. This mecanism is just used during training and not during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tender-suite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (Encoder_LSTM): EncoderLSTM(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(10537, 1024)\n",
      "    (LSTM): LSTM(1024, 2048, num_layers=2, dropout=0.5)\n",
      "  )\n",
      "  (Decoder_LSTM): DecoderLSTM(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(10537, 1024)\n",
      "    (LSTM): LSTM(1024, 2048, num_layers=2, dropout=0.5)\n",
      "    (fc): Linear(in_features=2048, out_features=9242, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, Encoder_LSTM, Decoder_LSTM, tfr=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder_LSTM = Encoder_LSTM\n",
    "        self.Decoder_LSTM = Decoder_LSTM\n",
    "        \n",
    "        self.tfr = tfr\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.Decoder_LSTM.output_size\n",
    "        \n",
    "        # Creating empty output tensor\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        hidden_state_encoder, cell_state_encoder = self.Encoder_LSTM(source)\n",
    "        \n",
    "        # Collecting a SOS token from the target in order to \"seed\" our decoder\n",
    "        x = target[0]\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            output, hidden_state_decoder, cell_state_decoder = self.Decoder_LSTM(x, hidden_state_encoder, cell_state_encoder)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            x = target[i] if random.random() < self.tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "# Constructing model\n",
    "\n",
    "# ENCODER\n",
    "input_voc_size = len(voc_french)\n",
    "encoder_embedding_size = 1024\n",
    "hidden_size = 2048\n",
    "encoder_num_layers = 2\n",
    "encoder_dropout = float(0.5)\n",
    "\n",
    "encoder_lstm = EncoderLSTM(input_voc_size, encoder_embedding_size, hidden_size, encoder_num_layers, encoder_dropout)\n",
    "\n",
    "# DECODER\n",
    "decoder_embedding_size = 1024\n",
    "hidden_size = 2048\n",
    "num_layers = 2\n",
    "decoder_dropout = float(0.5)\n",
    "output_size = len(voc_french)\n",
    "\n",
    "decoder_lstm = DecoderLSTM(len(voc_french), decoder_embedding_size, hidden_size, num_layers, decoder_dropout, len(voc_english))\n",
    "\n",
    "\n",
    "seq2seq_model = Seq2Seq(encoder_lstm, decoder_lstm)\n",
    "\n",
    "# Load model\n",
    "#seq2seq_model.load_state_dict(torch.load(\"./models/checkpoint-seq2seq-SD\"))\n",
    "\n",
    "print(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-parcel",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "artificial-poster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 / 1000\n",
      "torch.Size([19, 1])\n",
      "[1, 6589, 5652, 5652, 6657, 6657, 4154, 3393, 2709, 6508, 1255, 3420, 4086, 4086, 8372, 6727, 2054, 2054, 5232, 4959, 4959, 6916, 4476, 6916, 154, 6222, 6222, 1429, 4430, 1429, 6050, 4476, 8891, 2318, 2318, 8318, 2635, 2635, 2431, 825, 4391, 2635, 825, 9170, 9170, 3525, 3525, 3525, 4090, 7771, 6311]\n",
      "Source : Maintenant imaginez dans quelle mesure une telle chose pourrait aider à révolutionner votre domaine de travail.\n",
      "Translation : ['admire', 'acquire', 'acquire', 'purchase', 'purchase', 'rats', 'characteristics', 'economists', 'missions', 'governments', 'DH', 'plain', 'plain', 'rendering', 'anytime', 'analysis', 'analysis', 'grades', 'spinning', 'spinning', 'empowerment', 'label', 'empowerment', \"'ll\", 'blessed', 'blessed', 'shift', 'pockets', 'shift', 'Eve', 'label', 'separating', 'promise', 'promise', 'freeway', 'GPS', 'GPS', 'lay', 'Europe', 'approaching', 'GPS', 'Europe', 'portal', 'portal', 'symbols', 'symbols', 'symbols', 'rush', 'Scratch', 'translator']\n",
      "12 / 3422\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e7126e4e7ff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mseq2seq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{batch_idx} / {tot_batch}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-022127aaf7b6>\u001b[0m in \u001b[0;36mcollate_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0msrc_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m  \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from utils.seq2seq_utils import bleu, checkpoint_and_save, translate_sentence\n",
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = voc_english.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "epoch_loss = 0.0\n",
    "num_epochs = 1000\n",
    "best_loss = 999999\n",
    "best_epoch = -1\n",
    "test_sentence = \"Maintenant imaginez dans quelle mesure une telle chose pourrait aider à révolutionner votre domaine de travail.\"\n",
    "ts1 = []\n",
    "\n",
    "hist_seq2seq_loss = []\n",
    "hist_seq2seq_bleu = []\n",
    "\n",
    "tot_batch = len(train_dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "    # Print test sentences\n",
    "    seq2seq_model.eval()\n",
    "    \n",
    "    test_translation = translate_sentence(seq2seq_model, test_sentence, voc_english, voc_french, device, 50)\n",
    "    test_translation = \" \".join(test_translation)\n",
    "    \n",
    "    print(f\"Source : {test_sentence}\")\n",
    "    print(f\"Translation : {test_translation}\")\n",
    "    \n",
    "    seq2seq_model.train(True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        print(f\"{batch_idx} / {tot_batch}\", end='\\r')\n",
    "        \n",
    "        input = batch[0].to(device)\n",
    "        target = batch[1].to(device)\n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        output = model(input, target)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq_model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values using the gradients we calculated using bp \n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        epoch_loss += loss.item()\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        \n",
    "    epoch_loss = epoch_loss / len(train_iter)\n",
    "    \n",
    "    hist_seq2seq_loss.append(epoch_loss)\n",
    "    \n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_epoch = epoch\n",
    "        checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
    "        \n",
    "    if ((epoch - best_epoch) >= 10):\n",
    "        print(\"no improvement in 10 epochs, break\")\n",
    "        break\n",
    "        \n",
    "    print(\"Epoch_Loss - {}\".format(epoch_loss))\n",
    "        \n",
    "    score = bleu(test_data[:100], seq2seq_model, voc_english, voc_french, device)\n",
    "    print(f\"Bleu score {score*100:.2f}\")\n",
    "    hist_seq2seq_bleu.append(score)\n",
    "    \n",
    "    epoch_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-neutral",
   "metadata": {},
   "source": [
    "## Results and Conclusion\n",
    "\n",
    "The Seq2seq model is powerful enough to produce decent translation on relatively small sentences and was once the state of the art for language translation. As the model solely rely on the Context Vector to encode the whole sentence it quickly become a bottleneck to learn complexe seq2seq translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-lawyer",
   "metadata": {},
   "source": [
    "# Seq2Seq + Attention Mecanism\n",
    "\n",
    "To improve the performance of the Seq2Seq model a new mecanism called **Attention** was introduced. It leverage the output of the encoder's LSTM layers in order to provide more information to the decoder. It is called *Attention* as the architecture of the network encourage the decoder to learn on which part of the encoder's output it should focus. Here is a schema showing the attention module added to the decoder network to leverage the encoder outputs :\n",
    "\n",
    "![](./fig/attentionModule.png)\n",
    "\n",
    "Based on the previous hidden state of the decoder and the previous token generated, the attention module compute a vector called the *Attention weights* having the same length as the output of the decoder. By mutliplying the *Attention weights* vectore to the encoder outputs we obtain a new vector representing the attention of the network on the various part of the encoder output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-elizabeth",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder is the same as the one of the Seq2seq model. But some changes in the code are required to be able to retrieve the output for each LSTM step. In the Seq2Seq model we passed the whole sequence directly to the LSTM and pytorch only return the last output and hidden state. Now we have to collect the output of the LSTM at each timestep of the sequence forcing us to loop manually on the sequence, to collect the output and the hidden state of the LSTM step and we also have to take care of transmitting the previous hidden state to the next step.\n",
    "\n",
    "To be able to do that we just added an additional parameter to the *forward* methods representing the hidden state. We also added an *initHidden* method to generate an initial hidden state for the LSTM layer of the encoder. You will see how we use these different methods later in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttnEncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        # Size of the input vector\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Size of the word embedding\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # LSTM hidden layer size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Number of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initializing the network layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=dropout_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x, hidden)\n",
    "        \n",
    "        return outputs, hidden_state, cell_state\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-interstate",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is modified to add the attention mecanism to its process. The schema for the decoder with attention is the following :\n",
    "\n",
    "![](./fig/attentionDecoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttnDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p, output_size, max_len):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Input size of the decoder (size of the context vector)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Embedding size \n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Hidden unit size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # num of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Vocabulary size of the target language\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = dropout_p)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state, encoder_outputs):\n",
    "        \n",
    "        # Shape of [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        embedding = self.dropout(x)\n",
    "        \n",
    "        # Attention computation\n",
    "        attn_weights = self.softmax(self.attn(torch.cat((embedded[0], hidden_state), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(output, (hidden_state, cell_state))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden_state, cell_state\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-monitoring",
   "metadata": {},
   "source": [
    "## Attention Seq2Seq\n",
    "\n",
    "Now we explicitly loop on the input data in order to collect to outputs of the encoder and pass them as an additionnal parameter for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnSeq2Seq(nn.Module):\n",
    "    def __init__(self, Attn_Encoder_LSTM, Attn_Decoder_LSTM, max_length):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Attn_Encoder_LSTM = Attn_Encoder_LSTM\n",
    "        self.Attn_Decoder_LSTM = Attn_Decoder_LSTM\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        \n",
    "        # Processing the encoder part\n",
    "        \n",
    "        # Tensor to collect the outputs\n",
    "        encoder_outputs = torch.zeros(max_length, self.Attn_Encoder_LSTM.hidden_size).to(device)\n",
    "        \n",
    "        encoder_hidden = self.Attn_Encoder_LSTM.initHidden()\n",
    "        \n",
    "        input_len = source.shape[0]\n",
    "        for idx in range(input_len):\n",
    "            encoder_output, hidden_state, cell_state = self.Attn_Encoder_LSTM(source[idx], encoder_hidden)\n",
    "            encoder_hidden = (hidden_state, cell_state)\n",
    "            \n",
    "            encoder_outputs[idx] = encoder_output\n",
    "            \n",
    "        # Processing decoder\n",
    "        # Collecting a SOS token from the target in order to \"seed\" our decoder\n",
    "        x = target[0]\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            output, hidden_state_decoder, cell_state_decoder = self.Attn_Decoder_LSTM(x, hidden_state_encoder, cell_state_encoder, encoder_outputs)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-twist",
   "metadata": {},
   "source": [
    "# Transformer Network\n",
    "\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py\n",
    "\n",
    "https://github.com/Skumarr53/Attention-is-All-you-Need-PyTorch\n",
    "\n",
    "https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "\n",
    "\n",
    "Introduced under the name *The Transformer* in the paper [*Attention is All You Need*](https://arxiv.org/abs/1706.03762) the architecture propose to discard all the recurrent part to rely only on the attention mecanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-experiment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
