{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rising-samoa",
   "metadata": {},
   "source": [
    "# Introduction to Transformer Networks\n",
    "\n",
    "Transformer Network were introduced in 2017 in the paper [**Attention Is All You Need**](https://arxiv.org/abs/1706.03762). They are designed to handle sequential data (such as text) and could be use in sequence to sequence tasks. Unlike the RNN methods, Transformer Networks does not needs to see the inputs sequentially allowing them to be trained in parallel. Thus, Transformer Networks became the model of choice for NLP as it is now possible to train them on very large datasets.\n",
    "\n",
    "This Notebook aims to show, in practice, how we move from RNN to Transformer Networks for a Sequence to Sequence (Seq2Seq) task. The task chosen is language translation. This Notebook showcase three different methods :\n",
    "\n",
    "- **Seq2Seq RNN** : Our baseline model for language translation.\n",
    "- **Seq2Seq RNN with Attention Mecanism** : An improved version of the RNN model.\n",
    "- **Transformer Network** : The model introduced in *Attention Is All You Need* dropping the RNN part of the network and keeping only the attention mecanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "passive-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.dataloader import TranslationDataset\n",
    "\n",
    "# Simple RNN : https://keras.io/examples/nlp/lstm_seq2seq/ ou https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350\n",
    "# RNN + Attention : https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# Transformer : https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-firewall",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The model we showcase are trained for english to french translation. It is a classical Seq2Seq problem for which a lot of datasets exists. We chose to use the data available on the [manythings.org](https://www.manythings.org/anki/) website.\n",
    "\n",
    "First we load language models from [**Spacy**](https://spacy.io/), a python library for NLP. These language model are used to tokenize each sentences in our dataset. We also use the [**Torchtext**](https://pytorch.org/text/stable/index.html) library as it provides useful class for loading text datasets and generating the vocabulary.\n",
    "\n",
    "Once preprocessed, the sentences start with a special start of sentence token (*<sos>*) and end of sentence token (*<eos>*) and are transformed into a vector of numbers each number corresponding to one word in the vocabulary of the language.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "likely-findings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeromefink/.local/lib/python3.9/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/jeromefink/.local/lib/python3.9/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (fr) vocabulary: 9609\n",
      "Unique tokens in target (en) vocabulary: 6535\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import TranslationDataset\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Downloading vocabulary for our chosen languages\n",
    "!python -m spacy download en_core_web_sm --quiet\n",
    "!python -m spacy download fr_core_news_sm --quiet\n",
    "\n",
    "spacy_french = spacy.load(\"fr_core_news_sm\")\n",
    "spacy_english = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_french(text):\n",
    "    return [token.text for token in spacy_french.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)]\n",
    "\n",
    "\n",
    "french = Field(tokenize=tokenize_french, lower=True,\n",
    "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(tokenize=tokenize_english, lower=True,\n",
    "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "train_data, valid_data, test_data = TranslationDataset.splits(path=\"./data\", exts = (\".en\", \".fr\"),\n",
    "                                                    fields=(english, french))\n",
    "\n",
    "french.build_vocab(train_data, max_size=10000, min_freq=3)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=3)\n",
    "\n",
    "print(f\"Unique tokens in source (fr) vocabulary: {len(french.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specialized-strengthening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "English -  how 's work ?  Length -  4\n",
      "French -  comment va le travail ?  Length -  5\n",
      "\n",
      "English -  hurry back .  Length -  3\n",
      "French -  reviens vite .  Length -  3\n",
      "\n",
      "English -  hurry home .  Length -  3\n",
      "French -  dépêche - toi d' aller chez toi !  Length -  8\n",
      "\n",
      "English -  hurry home .  Length -  3\n",
      "French -  dépêchez -vous d' aller chez vous !  Length -  7\n",
      "\n",
      "Maximum Length of English Sentence 48 and French Sentence 57 in the dataset\n",
      "Minimum Length of English Sentence 2 and French Sentence 2 in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeromefink/.local/lib/python3.9/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Creating the iterator and print sample\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE,\n",
    "                                                                      sort_within_batch=True,\n",
    "                                                                      sort_key=lambda x: len(x.src),\n",
    "                                                                      device = device)\n",
    "max_len_fr = []\n",
    "max_len_en = []\n",
    "count = 0\n",
    "\n",
    "for data in train_data:\n",
    "    max_len_en.append(len(data.src))\n",
    "    max_len_fr.append(len(data.trg))\n",
    "    if count > 1000 and count < 1005 :\n",
    "        print(\"English - \",*data.src, \" Length - \", len(data.src))\n",
    "        print(\"French - \",*data.trg, \" Length - \", len(data.trg))\n",
    "        print()\n",
    "    count += 1\n",
    "\n",
    "print(\"Maximum Length of English Sentence {} and French Sentence {} in the dataset\".format(max(max_len_en),max(max_len_fr)))\n",
    "print(\"Minimum Length of English Sentence {} and French Sentence {} in the dataset\".format(min(max_len_en),min(max_len_fr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-marshall",
   "metadata": {},
   "source": [
    "# Seq2Seq RNN\n",
    "\n",
    "A Seq2Seq RNN is made of two component. An encoder network evaluating the input sequence and generating a vector representing the sentence called the **Context Vector**. The context vector is then passed to a decoder network that will construct the input sequence.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The input of the encoder is the tokenized sentence with the start and end of sentence token. The purpose of the encoder is to create a context vector containing all the information needed by the decoder to reconstitute the translated sentence. To process the sequence of word token, we use LSTM layer and the hidden state of the last LSTM layer is used as the context vector.\n",
    "\n",
    "![](./fig/seq2seq-encoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "computational-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        # Size of the input vector\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Size of the word embedding\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # LSTM hidden layer size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Number of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initializing the network layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=dropout_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x)\n",
    "        \n",
    "        return hidden_state, cell_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-mission",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "As the context vector issued by the encoder are the hidden state of LSTM, the decoder could use those context vector as the initial hidden state of its LSTM unit. In order to predict the next word in the sentence, the decoder network could use the information contained in the hiddent state of its LSTM units and the previous word it predicted :\n",
    "\n",
    "![](./fig/seq2seq-decoder.png)\n",
    "\n",
    "The first call of the decoder is initialize with the context vector and the start of sentence token. Then, the decoder use the previous token it emits as an input and could rely on its hidden state to convey the remaining contextual information about the sentence. The decoder is expected to emit the end of sentence token once he reached the end of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        \n",
    "        # Input size of the decoder (size of the context vector)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Embedding size \n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Hidden unit size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # num of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Vocabulary size of the target language\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = dropout_p)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \n",
    "        # Shape of [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x, (hidden_state, cell_state))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden_state, cell_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-narrow",
   "metadata": {},
   "source": [
    "## Joining Encoder and Decoder\n",
    "\n",
    "The final Seq2seq model could be built by stacking the encoder and the decoder network. But a regularization mecanism is added in the final seq2seq model to ease the learning tasks. The regularization is called the **Teach Force Ration** (tfr) and aims to correct the decoder by providing the actual token from the target sentence as an input instead of reusing the previously predicted token. Here is an example of the full seq2seq network :\n",
    "\n",
    "![](./fig/seq2seq.png)\n",
    "\n",
    "We just pass the context vector as the initial hidden state of the decoder and add the TFR mecanism that consist of providing the real target token expected as input of the LSTM instead of the previously predicted token with a probability of 50%. This mecanism is just used during training and not during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "color-operator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (Encoder_LSTM): EncoderLSTM(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(6535, 300)\n",
      "    (LSTM): LSTM(300, 2048, num_layers=2, dropout=0.5)\n",
      "  )\n",
      "  (Decoder_LSTM): DecoderLSTM(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(9609, 300)\n",
      "    (LSTM): LSTM(300, 2048, num_layers=2, dropout=0.5)\n",
      "    (fc): Linear(in_features=2048, out_features=9609, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder_LSTM = Encoder_LSTM\n",
    "        self.Decoder_LSTM = Decoder_LSTM\n",
    "        \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.Decoder_LSTM.output_size\n",
    "        \n",
    "        # Creating empty output tensor\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        hidden_state_encoder, cell_state_encoder = self.Encoder_LSTM(source)\n",
    "        \n",
    "        # Collecting a SOS token from the target in order to \"seed\" our decoder\n",
    "        x = target[0]\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            output, hidden_state_decoder, cell_state_decoder = self.Decoder_LSTM(x, hidden_state_encoder, cell_state_encoder)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "# Constructing model\n",
    "\n",
    "# ENCODER\n",
    "input_voc_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "hidden_size = 2048\n",
    "encoder_num_layers = 2\n",
    "encoder_dropout = float(0.5)\n",
    "\n",
    "encoder_lstm = EncoderLSTM(input_voc_size, encoder_embedding_size, hidden_size, encoder_num_layers, encoder_dropout).to(device)\n",
    "\n",
    "# DECODER\n",
    "french_voc_size = len(french.vocab)\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 2048\n",
    "num_layers = 2\n",
    "decoder_dropout = float(0.5)\n",
    "output_size = len(english.vocab)\n",
    "\n",
    "decoder_lstm = DecoderLSTM(french_voc_size, decoder_embedding_size, hidden_size, num_layers, decoder_dropout, french_voc_size).to(device)\n",
    "\n",
    "\n",
    "seq2seq_model = Seq2Seq(encoder_lstm, decoder_lstm)\n",
    "\n",
    "print(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-wichita",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 / 100\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " retournes servi souci souci enquis vicieux avertir lacet matin paresse paresse crier outré payer ressentent organisation intéressants intéressants intéressants intéressants cadre cadre bio parfois curiosité chaude économisé alentour alentour lacet petite lacet petite paresse crier exagérer ressentent organisation zéro traduis ressentent organisation zéro traduis autorisa autorisa autorisa application application marins\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 4.966594655898838\n",
      "Bleu score 0.00\n",
      "Epoch - 2 / 100\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez le <unk> . <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 3.642369503008131\n",
      "Bleu score 0.00\n",
      "Epoch - 3 / 100\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez s' il te plaît . <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 2.9793751108687356\n",
      "Bleu score 6.55\n",
      "Epoch - 4 / 100\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez s' il vous plaît <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 2.5741525123755684\n",
      "Bleu score 6.10\n",
      "Epoch - 5 / 100\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez s' il vous prie <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 2.293644495982621\n",
      "Bleu score 4.13\n",
      "Epoch - 6 / 100\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez s' il y <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 2.0984092991786314\n",
      "Bleu score 1.24\n",
      "Epoch - 7 / 100\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez signer , <eos>\n",
      "65 / 873\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from utils.seq2seq_utils import bleu, checkpoint_and_save, translate_sentence\n",
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = french.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "epoch_loss = 0.0\n",
    "num_epochs = 100\n",
    "best_loss = 999999\n",
    "best_epoch = -1\n",
    "test_sentence = \"Please write down your name, address, and phone number here.\"\n",
    "ts1 = []\n",
    "\n",
    "tot_batch = len(train_iterator)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "    \n",
    "    seq2seq_model.eval()\n",
    "    translated_test_sentence = translate_sentence(seq2seq_model, test_sentence, english, french, device, max_length=50)\n",
    "    translated_test_sentence = \" \".join(translated_test_sentence)\n",
    "    print(f\"Source : \\n {test_sentence}\")\n",
    "    print(f\"Translation : \\n {translated_test_sentence}\")\n",
    "    ts1.append(translated_test_sentence)\n",
    "    seq2seq_model.train(True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        print(f\"{batch_idx} / {tot_batch}\", end='\\r')\n",
    "        \n",
    "        input = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        output = model(input, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq_model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values using the gradients we calculated using bp \n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        epoch_loss += loss.item()\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        \n",
    "    epoch_loss = epoch_loss / len(train_iterator)\n",
    "    \n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_epoch = epoch\n",
    "        checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
    "        \n",
    "    if ((epoch - best_epoch) >= 10):\n",
    "        print(\"no improvement in 10 epochs, break\")\n",
    "        break\n",
    "        \n",
    "    print(\"Epoch_Loss - {}\".format(epoch_loss))\n",
    "        \n",
    "\n",
    "    score = bleu(test_data[100:600], seq2seq_model, english, french, device)\n",
    "    print(f\"Bleu score {score*100:.2f}\")\n",
    "    \n",
    "    epoch_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-performance",
   "metadata": {},
   "source": [
    "## Results and Conclusion\n",
    "\n",
    "The Seq2seq model is powerful enough to produce decent translation on relatively small sentences and was once the state of the art for language translation. However, the main weakness of the architecture is that we expect the encoder to be able to encode the whole sentence to translate in the final context vector and the decoder is expected to be able to use only the information contained in that vector in order to produce the final sentence. \n",
    "\n",
    "The context vector quickly become the bottleneck of seq2seq network as complex and long sentences could not be efficiently encoded and, thus, results in poor translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-thickness",
   "metadata": {},
   "source": [
    "# Seq2Seq + Attention Mecanism\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
