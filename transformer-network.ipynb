{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binding-bibliography",
   "metadata": {},
   "source": [
    "# Introduction to Transformer Networks\n",
    "\n",
    "Transformer Network were introduced in 2017 in the paper [**Attention Is All You Need**](https://arxiv.org/abs/1706.03762). They are designed to handle sequential data (such as text) and could be use in sequence to sequence tasks. Unlike the RNN methods, Transformer Networks does not needs to see the inputs sequentially allowing them to be trained in parallel. Thus, Transformer Networks became the model of choice for NLP as it is now possible to train them on very large datasets.\n",
    "\n",
    "This Notebook aims to show, in practice, how we move from RNN to Transformer Networks for a Sequence to Sequence (Seq2Seq) task. The task chosen is language translation. This Notebook showcase three different methods :\n",
    "\n",
    "- **Seq2Seq RNN** : Our baseline model for language translation.\n",
    "- **Seq2Seq RNN with Attention Mecanism** : An improved version of the RNN model.\n",
    "- **Transformer Network** : The model introduced in *Attention Is All You Need* dropping the RNN part of the network and keeping only the attention mecanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expensive-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simple RNN : https://keras.io/examples/nlp/lstm_seq2seq/ ou https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350\n",
    "# RNN + Attention : https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# Transformer : https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-crisis",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The model we showcase are trained for english to french translation. It is a classical Seq2Seq problem for which a lot of datasets exists. We chose to use the data available on the [manythings.org](https://www.manythings.org/anki/) website.\n",
    "\n",
    "First we load language models from [**Spacy**](https://spacy.io/), a python library for NLP. These language model are used to tokenize each sentences in our dataset. We also use the [**Torchtext**](https://pytorch.org/text/stable/index.html) library as it provides useful class for loading text datasets and generating the vocabulary.\n",
    "\n",
    "Once preprocessed, the sentences start with a special start of sentence token (*<sos>*) and end of sentence token (*<eos>*) and are transformed into a vector of numbers each number corresponding to one word in the vocabulary of the language.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "friendly-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeromefink/.local/lib/python3.9/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/jeromefink/.local/lib/python3.9/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (fr) vocabulary: 9609\n",
      "Unique tokens in target (en) vocabulary: 6535\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import TranslationDataset\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Downloading vocabulary for our chosen languages\n",
    "!python -m spacy download en_core_web_sm --quiet\n",
    "!python -m spacy download fr_core_news_sm --quiet\n",
    "\n",
    "spacy_french = spacy.load(\"fr_core_news_sm\")\n",
    "spacy_english = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_french(text):\n",
    "    return [token.text for token in spacy_french.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)]\n",
    "\n",
    "\n",
    "french = Field(tokenize=tokenize_french, lower=True,\n",
    "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(tokenize=tokenize_english, lower=True,\n",
    "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "train_data, valid_data, test_data = TranslationDataset.splits(path=\"./data\", exts = (\".en\", \".fr\"),\n",
    "                                                    fields=(english, french))\n",
    "\n",
    "french.build_vocab(train_data, max_size=10000, min_freq=3)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=3)\n",
    "\n",
    "print(f\"Unique tokens in source (fr) vocabulary: {len(french.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "selective-delight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "English -  how 's work ?  Length -  4\n",
      "French -  comment va le travail ?  Length -  5\n",
      "\n",
      "English -  hurry back .  Length -  3\n",
      "French -  reviens vite .  Length -  3\n",
      "\n",
      "English -  hurry home .  Length -  3\n",
      "French -  dépêche - toi d' aller chez toi !  Length -  8\n",
      "\n",
      "English -  hurry home .  Length -  3\n",
      "French -  dépêchez -vous d' aller chez vous !  Length -  7\n",
      "\n",
      "Maximum Length of English Sentence 48 and French Sentence 57 in the dataset\n",
      "Minimum Length of English Sentence 2 and French Sentence 2 in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeromefink/.local/lib/python3.9/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Creating the iterator and print sample\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE,\n",
    "                                                                      sort_within_batch=True,\n",
    "                                                                      sort_key=lambda x: len(x.src),\n",
    "                                                                      device = device)\n",
    "max_len_fr = []\n",
    "max_len_en = []\n",
    "count = 0\n",
    "\n",
    "for data in train_data:\n",
    "    max_len_en.append(len(data.src))\n",
    "    max_len_fr.append(len(data.trg))\n",
    "    if count > 1000 and count < 1005 :\n",
    "        print(\"English - \",*data.src, \" Length - \", len(data.src))\n",
    "        print(\"French - \",*data.trg, \" Length - \", len(data.trg))\n",
    "        print()\n",
    "    count += 1\n",
    "\n",
    "print(\"Maximum Length of English Sentence {} and French Sentence {} in the dataset\".format(max(max_len_en),max(max_len_fr)))\n",
    "print(\"Minimum Length of English Sentence {} and French Sentence {} in the dataset\".format(min(max_len_en),min(max_len_fr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-completion",
   "metadata": {},
   "source": [
    "# Seq2Seq RNN\n",
    "\n",
    "A Seq2Seq RNN is made of two component. An encoder network evaluating the input sequence and generating a vector representing the sentence called the **Context Vector**. The context vector is then passed to a decoder network that will construct the input sequence.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The input of the encoder is the tokenized sentence with the start and end of sentence token. The purpose of the encoder is to create a context vector containing all the information needed by the decoder to reconstitute the translated sentence. To process the sequence of word token, we use LSTM layer and the hidden state of the last LSTM layer is used as the context vector.\n",
    "\n",
    "![](./fig/seq2seq-encoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "architectural-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        # Size of the input vector\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Size of the word embedding\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # LSTM hidden layer size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Number of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initializing the network layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=dropout_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x)\n",
    "        \n",
    "        return hidden_state, cell_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-jesus",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "As the context vector issued by the encoder are the hidden state of LSTM, the decoder could use those context vector as the initial hidden state of its LSTM unit. In order to predict the next word in the sentence, the decoder network could use the information contained in the hiddent state of its LSTM units and the previous word it predicted :\n",
    "\n",
    "![](./fig/seq2seq-decoder.png)\n",
    "\n",
    "The first call of the decoder is initialize with the context vector and the start of sentence token. Then, the decoder use the previous token it emits as an input and could rely on its hidden state to convey the remaining contextual information about the sentence. The decoder is expected to emit the end of sentence token once he reached the end of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mexican-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        \n",
    "        # Input size of the decoder (size of the context vector)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Embedding size \n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Hidden unit size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # num of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Vocabulary size of the target language\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = dropout_p)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \n",
    "        # Shape of [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x, (hidden_state, cell_state))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden_state, cell_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-clear",
   "metadata": {},
   "source": [
    "## Joining Encoder and Decoder\n",
    "\n",
    "The final Seq2seq model could be built by stacking the encoder and the decoder network. But a regularization mecanism is added in the final seq2seq model to ease the learning tasks. The regularization is called the **Teach Force Ration** (tfr) and aims to correct the decoder by providing the actual token from the target sentence as an input instead of reusing the previously predicted token. Here is an example of the full seq2seq network :\n",
    "\n",
    "![](./fig/seq2seq.png)\n",
    "\n",
    "We just pass the context vector as the initial hidden state of the decoder and add the TFR mecanism that consist of providing the real target token expected as input of the LSTM instead of the previously predicted token with a probability of 50%. This mecanism is just used during training and not during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "trying-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (Encoder_LSTM): EncoderLSTM(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(6535, 300)\n",
      "    (LSTM): LSTM(300, 2048, num_layers=2, dropout=0.5)\n",
      "  )\n",
      "  (Decoder_LSTM): DecoderLSTM(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(9609, 300)\n",
      "    (LSTM): LSTM(300, 2048, num_layers=2, dropout=0.5)\n",
      "    (fc): Linear(in_features=2048, out_features=9609, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder_LSTM = Encoder_LSTM\n",
    "        self.Decoder_LSTM = Decoder_LSTM\n",
    "        \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.Decoder_LSTM.output_size\n",
    "        \n",
    "        # Creating empty output tensor\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        hidden_state_encoder, cell_state_encoder = self.Encoder_LSTM(source)\n",
    "        \n",
    "        # Collecting a SOS token from the target in order to \"seed\" our decoder\n",
    "        x = target[0]\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            output, hidden_state_decoder, cell_state_decoder = self.Decoder_LSTM(x, hidden_state_encoder, cell_state_encoder)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "# Constructing model\n",
    "\n",
    "# ENCODER\n",
    "input_voc_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "hidden_size = 2048\n",
    "encoder_num_layers = 2\n",
    "encoder_dropout = float(0.5)\n",
    "\n",
    "encoder_lstm = EncoderLSTM(input_voc_size, encoder_embedding_size, hidden_size, encoder_num_layers, encoder_dropout).to(device)\n",
    "\n",
    "# DECODER\n",
    "french_voc_size = len(french.vocab)\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 2048\n",
    "num_layers = 2\n",
    "decoder_dropout = float(0.5)\n",
    "output_size = len(english.vocab)\n",
    "\n",
    "decoder_lstm = DecoderLSTM(french_voc_size, decoder_embedding_size, hidden_size, num_layers, decoder_dropout, french_voc_size).to(device)\n",
    "\n",
    "\n",
    "seq2seq_model = Seq2Seq(encoder_lstm, decoder_lstm)\n",
    "\n",
    "print(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-language",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-joining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 0.00\n",
      "Epoch - 15 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez vous en prie <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.6856157862307144\n",
      "Bleu score 0.00\n",
      "Epoch - 16 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez lui dire à <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.6265845902323859\n",
      "Bleu score 0.00\n",
      "Epoch - 17 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez à mon <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.5800690896049534\n",
      "Bleu score 1.08\n",
      "Epoch - 18 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez me le <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.5553015100450722\n",
      "Bleu score 1.03\n",
      "Epoch - 19 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez je suis suis -ce tu ne ne ne est est est a a a <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.5149828655067041\n",
      "Bleu score 0.76\n",
      "Epoch - 20 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez je suis ne suis en suis suis en en <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.4677458490839255\n",
      "Bleu score 0.00\n",
      "Epoch - 21 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez je suis suis en pas de <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.4599885132168031\n",
      "Bleu score 0.00\n",
      "Epoch - 22 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez je me suis me je suis suis me suis me me le <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.434498054601619\n",
      "Bleu score 0.00\n",
      "Epoch - 23 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " veuillez je suis suis me suis me le suis en pas de vous vous l' l' <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.406134639204163\n",
      "Bleu score 0.00\n",
      "Epoch - 24 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez je suis suis me suis me le ai pas me le <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.3804449641144698\n",
      "Bleu score 0.00\n",
      "Epoch - 25 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez je suis suis vous ne ne vous vous en en en <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.3419690949400676\n",
      "Bleu score 0.00\n",
      "Epoch - 26 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez je suis me suis suis suis je ne ne vous vous l' l' as as as <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.3279530310412315\n",
      "Bleu score 0.00\n",
      "Epoch - 27 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez je suis me le dis vous le en <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.314504265853493\n",
      "Bleu score 0.00\n",
      "Epoch - 28 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez je suis ne ne vous vous as as a <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.2909640193941394\n",
      "Bleu score 0.54\n",
      "Epoch - 29 / 1000\n",
      "Source : \n",
      " Please write down your name, address, and phone number here.\n",
      "Translation : \n",
      " écrivez à <eos>\n",
      "saving873\n",
      "\n",
      "Epoch_Loss - 1.2861705676386856\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from utils.seq2seq_utils import bleu, checkpoint_and_save, translate_sentence\n",
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = french.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "epoch_loss = 0.0\n",
    "num_epochs = 1000\n",
    "best_loss = 999999\n",
    "best_epoch = -1\n",
    "test_sentence = \"Please write down your name, address, and phone number here.\"\n",
    "ts1 = []\n",
    "\n",
    "hist_seq2seq_loss = []\n",
    "hist_seq2seq_bleu = []\n",
    "\n",
    "tot_batch = len(train_iterator)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "    \n",
    "    seq2seq_model.eval()\n",
    "    translated_test_sentence = translate_sentence(seq2seq_model, test_sentence, english, french, device, max_length=50)\n",
    "    translated_test_sentence = \" \".join(translated_test_sentence)\n",
    "    print(f\"Source : \\n {test_sentence}\")\n",
    "    print(f\"Translation : \\n {translated_test_sentence}\")\n",
    "    ts1.append(translated_test_sentence)\n",
    "    seq2seq_model.train(True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        print(f\"{batch_idx} / {tot_batch}\", end='\\r')\n",
    "        \n",
    "        input = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        output = model(input, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq_model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values using the gradients we calculated using bp \n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        epoch_loss += loss.item()\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        \n",
    "    epoch_loss = epoch_loss / len(train_iterator)\n",
    "    \n",
    "    hist_seq2seq_loss.append(epoch_loss)\n",
    "    \n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_epoch = epoch\n",
    "        checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
    "        \n",
    "    if ((epoch - best_epoch) >= 10):\n",
    "        print(\"no improvement in 10 epochs, break\")\n",
    "        break\n",
    "        \n",
    "    print(\"Epoch_Loss - {}\".format(epoch_loss))\n",
    "        \n",
    "\n",
    "    score = bleu(test_data[100:600], seq2seq_model, english, french, device)\n",
    "    print(f\"Bleu score {score*100:.2f}\")\n",
    "    hist_seq2seq_bleu.append(score)\n",
    "    \n",
    "    epoch_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-scotland",
   "metadata": {},
   "source": [
    "## Results and Conclusion\n",
    "\n",
    "The Seq2seq model is powerful enough to produce decent translation on relatively small sentences and was once the state of the art for language translation. As the model solely rely on the Context Vector to encode the whole sentence it quickly become a bottleneck to learn complexe seq2seq translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-pontiac",
   "metadata": {},
   "source": [
    "# Seq2Seq + Attention Mecanism\n",
    "\n",
    "To improve the performance of the Seq2Seq model a new mecanism called **Attention** was introduced. It leverage the output of the encoder's LSTM layers in order to provide more information to the decoder. It is called *Attention* as the architecture of the network encourage the decoder to learn on which part of the encoder's output it should focus. Here is a schema showing the attention module added to the decoder network to leverage the encoder outputs :\n",
    "\n",
    "![](./fig/attentionModule.png)\n",
    "\n",
    "Based on the previous hidden state of the decoder and the previous token generated, the attention module compute a vector called the *Attention weights* having the same length as the output of the decoder. By mutliplying the *Attention weights* vectore to the encoder outputs we obtain a new vector representing the attention of the network on the various part of the encoder output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-newport",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder is the same as the one of the Seq2seq model. But some changes in the code are required to be able to retrieve the output for each LSTM step. In the Seq2Seq model we passed the whole sequence directly to the LSTM and pytorch only return the last output and hidden state. Now we have to collect the output of the LSTM at each timestep of the sequence forcing us to loop manually on the sequence, to collect the output and the hidden state of the LSTM step and we also have to take care of transmitting the previous hidden state to the next step.\n",
    "\n",
    "To be able to do that we just added an additional parameter to the *forward* methods representing the hidden state. We also added an *initHidden* method to generate an initial hidden state for the LSTM layer of the encoder. You will see how we use these different methods later in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttnEncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        # Size of the input vector\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Size of the word embedding\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # LSTM hidden layer size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Number of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initializing the network layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=dropout_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(x, hidden)\n",
    "        \n",
    "        return outputs, hidden_state, cell_state\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-brisbane",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is modified to add the attention mecanism to its process. The schema for the decoder with attention is the following :\n",
    "\n",
    "![](./fig/attentionDecoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttnDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p, output_size, max_len):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Input size of the decoder (size of the context vector)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Embedding size \n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Hidden unit size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # num of LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Vocabulary size of the target language\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = dropout_p)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state, encoder_outputs):\n",
    "        \n",
    "        # Shape of [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        embedding = self.dropout(x)\n",
    "        \n",
    "        # Attention computation\n",
    "        attn_weights = self.softmax(self.attn(torch.cat((embedded[0], hidden_state), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(output, (hidden_state, cell_state))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden_state, cell_state\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-moscow",
   "metadata": {},
   "source": [
    "## Attention Seq2Seq\n",
    "\n",
    "Now we explicitly loop on the input data in order to collect to outputs of the encoder and pass them as an additionnal parameter for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnSeq2Seq(nn.Module):\n",
    "    def __init__(self, Attn_Encoder_LSTM, Attn_Decoder_LSTM, max_length):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Attn_Encoder_LSTM = Attn_Encoder_LSTM\n",
    "        self.Attn_Decoder_LSTM = Attn_Decoder_LSTM\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        \n",
    "        # Processing the encoder part\n",
    "        \n",
    "        # Tensor to collect the outputs\n",
    "        encoder_outputs = torch.zeros(max_length, self.Attn_Encoder_LSTM.hidden_size).to(device)\n",
    "        \n",
    "        encoder_hidden = self.Attn_Encoder_LSTM.initHidden()\n",
    "        \n",
    "        input_len = source.shape[0]\n",
    "        for idx in range(input_len):\n",
    "            encoder_output, hidden_state, cell_state = self.Attn_Encoder_LSTM(source[idx], encoder_hidden)\n",
    "            encoder_hidden = (hidden_state, cell_state)\n",
    "            \n",
    "            encoder_outputs[idx] = encoder_output\n",
    "            \n",
    "        # Processing decoder\n",
    "        # Collecting a SOS token from the target in order to \"seed\" our decoder\n",
    "        x = target[0]\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            output, hidden_state_decoder, cell_state_decoder = self.Attn_Decoder_LSTM(x, hidden_state_encoder, cell_state_encoder, encoder_outputs)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
